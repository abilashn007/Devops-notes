Kubernetes Installation 
	Minimum requirement for K8S master node is (2-core CPU and 2GB Ram)
	
	1. sudo apt update 
	2. sudo apt-get install -y apt-transport-https
	3. sudo su -
	4. curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add
	5. echo 'deb http://apt.kubernetes.io/ kubernetes-xenial main' > /etc/apt/sources.list.d/kubernetes.list
	6. exit from sudo 
	7. sudo apt update 
	8. sudo apt install -y docker.io
	
	9. sudo systemctl start docker 
	10. sudo systemctl enable docker.service 
	
	11. sudo apt-get install -y kubelet kubeadm kubectl kubernetes-cni
	
	12. Login back to master node,  make sure below steps are executed before running kubeadm init 
		1. sudo su - 
		2. docker cgroup driver configuration need to be updated 
			1. add the below content to the file /etc/docker/daemon.json
				{
				  "exec-opts": ["native.cgroupdriver=systemd"]
				}
			2. systemctl daemon-reload
			   systemctl restart docker 

			   systemctl restart kubectl (optional for first time)	
	
	Take ami from the above ec2 instances to create worker nodes 
		
	13. kubeadm init 
			if this command executes successfully then we get kubeadm join command with token
			save this command in seperate file for worker nodes to add to this master.

	14. k8s configurations for kubectl command 
		1. exit from root 
		2. copy the default k8s conf file to home 
			a. mkdir -p $HOME/.kube
			b. sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
			c. sudo chown $(id -u):$(id -g) $HOME/.kube/config

	15. For now open all ports in master 
		
	16. Now Install k8s CNI driver
			1. sudo sysctl net.bridge.bridge-nf-call-iptables=1
			2. kubectl apply -f "https://cloud.weave.works/k8s/v1.13/net.yaml"
						
	17. Login to worker nodes 
		a. sudo su -
		
		b. systemctl daemon-reload 
		   systemctl restart docker 
	           systemctl restart kubectl 	
			
		c. Run the kubeadm join <TOKEN> command which we get from kubeadm init from master 
		
	18. In master node check for the worker nodes.
		kubectl get nodes 
			
			
kubernetes Architecture 	
	The architecture of k8s differs from master and worker node 

	Master node components 
		1. Api Server / kube-api-server
			- It is the main management point of the cluster and also called 
			  as brain of the cluster.
			- All the components are directly connected to API serve, they 
			  communicate through API server only and no other component will 
			  communicate directly with each other.
			- This is the only component which connects and got access to etcd.
			- All the cluster requests are authenticated and authorised by API server.
			- API server has a watch mechanism for watching the changes in cluster.
			
		2. etcd 
			- ectd is a distributed , consistent key value store used for 
			  storing the complete cluster information/data.
			- ectd contains data such as configuration management of cluster,
              distributed work and basically complete cluster information.			
			
		3. scheduler / kube-scheduler
			- The scheduler always watches for a new pod request and 
			  decides which worker node this pod should be created.
			- Based on the worker node load, affinity and anti-affiny, taint configuration 
			  pod will be scheduled to a particular node.
			  
		Controller manager /control manager / kube-controller 
			- It is a daemon that always runs and embeds core control loops known as controllers. 
			- K8s has some inbuild controllers such as Deployment, DaemonSet, ReplicaSet, Replication controller,
			  node controller, jobs, cronjob, endpoint controller, namespace controller etc.	
			
		Cloud controller manager 
			- These controller help us to connect with the public cloud provider service and this component 
			  is maintained by cloud providers only.

Worker node components 
		kubelet 
			- It is an agent that runs on each and every worker node and it alsways watches the API 
			  server for pod related changes running in its worker node.
			- kubelet always make sure that the assigend pods to its worker node is running.
			- kubelet is the one which communicates with containarisation tool (docker daemon)
              		  through docker API (CRI). 	
			- work of kubelet is to create and run the pods. Always reports the status of the worker node 
			  and each pod to API server. (uses a tool call cAdvisor)
			- Kubelet is the one which runs probes.	
		
		kube service proxy 
			(in k8s service means networking)
			- Service proxy runs on each and every worker node and is responsble for watching API 
			  server for any changes in service configuration (any network related configuration).	
			- Based on the configuration service proxy manages the entire network of worker node.

		Container runtime interface (CRI)
			- This component initialy identifies the container technology and connects it to kubelet.
			
			
		pod
			- pods are the smallest deployable object in kuberntes.
			- pod should contain atleast one container and can have n number of containers.
			- If pod contains more than one container all the container share the same memory assigned to that pod.

YAML file 
	- filetype .yaml or .yml 
	- YAML file contains key - value pairs where key are fixed and defined by the 
	  kubernetes and value is user defined configuration.
	- Values supoorts multiple datatypes - string, Integer, Boolean, Array, List.	
	
	example: List representation  
	
			1) name: Harsha
			   hobbies: ["Driving","coding"]
					
						(or)
				
			   name: Harsha
			   hobbies: 
				   - Driving
				   - coding

k8s yaml syntax example with pod:
	
	apiVersion: v1
	kind: Pod 
	metadata: 
    	    name: my-first-pod	
	spec: 
    	    containers:
       		- name: my-nginx 
         	  image: nginx:latest
		  ports: 
		    - containerPort: 80 	
	
	apiVersion: v1
		- This is the version of api used to create a k8s object.
		- The fields are case-sensitive and YAML use camelcase.
		- The type of api are alpha, beta and stable.
		
	kind: Pod
		- here we specify which object we need to create. 
		- Always object name first letter is capital.
		
	metadata:
	    - This field is used to provide information on the object 
		  which we are creating.
		- Information such as name, labels and annotations. 	
	
	spec:
		- This is used to do the actual configuration of the 
		  object.

		 ports: 
		    - containerPort: 80
	

To create / apply a configuration 
	kubectl apply -f <file>.yml	
	
To list objects 
	kubectl get <obeject_type>
		ex: List pods - kubectl get pods 
		    List deployment - kubectl get deployments
			
To delete objects 
	kubectl delete <object_type>


Assignment: What happens if we create a pod with kubectl ?		  

K8S Labels and selectors 
	- K8S labels is a metadata key value which can be applied to any object in k8s.
	- Labels are used to identify by using selectors.
	- Multiple objects can have same label, multiple labels to same object and Label length should be less that 63 characters.
	
	TO list all labels of a object 
		kubectl get <obeject_type> <obejct_name> --show-labels 
	
	
	Selectors 
		- Selectors are used to filter and identifly the labeled k8s object.
		
		Equality-Based 
			- It will use only one label in comparision and it will look for objects with exact same 
			  string in label.
			- we can use 3 types of operators equal ( = or == ) and not-qual ( != )	
		
			example: 
				selectors: 
					matchLabels: 
						app=nginx 
						   (or)
						app: nginx   
		
		set-based 
			- This type of selector allows us to filter objects based on multiple set of values to a label key.
			- 3 types of operators we can use in, notin and exists.

				example: 
					selectors: 
						matchLabels: 
							app in (nginx, my-nginx)
							app exits (nginx, my-nginx)
							app notin (nginx, my-nginx)
Annotations 
	- These are used for record purpose only and to provide some user information to objects.
	- These are non-identifying metadata so we cannot use selectors on annotations.

	example: personal_info, phone_number, imageregistry, author	
	
Assignment: Difference b/w set-based and equality-based selector.
			Difference b/w labels and annotations.

ReplicaSet vs Replication Controller
	- Both ensures that a specified number of pod replicas are alyways running at a given point of time.
	- Replication controller is a very old way of replicating the pos and now it is replaced by ReplicaSet
      
	- The only differenece b/w them is their selector types.
		Replication Controller supports only equality-based selector. 
		ReplicaSet supports both equality-based and set-based selectors.

Deployment controller / Deployment / k8s deployment 
	- Deployment is used to create replicas of pod and it makes sure at a given point of time 
	  the number of replicas of pod is alway running. 
	- Deployment internally uses ReplicaSet to replicate the pods.
	- If we update the configuration in deployment it will automatically updates it to all the pods.
	- Rollout and Rollback of pod update is possible.
	- we can pause a deployment whenerver we need.
	- Deployment has got its own internal autoscaller which is of type horizontal scaller. 
		To apply calling 
			kubectl autoscale deployment.v1.apps/<deployment_name> --min=5 --max=20 --cpu-percent=50	
	- scaleup and scaledown is possible by increasing and decreasing the replica count at any given 
	  point of time.
		kubectl scale deployment.v1.apps/<deployment_name> --replicas=10	
	
	- deployment is a cluster level object.
	
		deployment = pod + ReplicaSet + autoscaling + RollingUpdates 
	
	Deployment spec file.
		
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: nginx-deployment-new
		  labels:
			app: my-deployment-nginx
		spec:
		  replicas: 5
		  selector:
			matchLabels:
			  app=nginx
		  template:
			metadata:
			  labels:
				app: nginx
			spec:
			  containers:
			  - name: nginx
				image: nginx:1.14.2
				ports:
				- containerPort: 80
	
Assignment: demo on selectors types 
		  
	
DaemonSet
	- DaemonSet ensures that a copy of pod is always running on all the worker nodes in the cluster.
	-If a new node is added or if deleted DaemonSet will automatically adds/deletes the pod.

	usage: 
		- we use DaemonSet to deploy monitoring agents in every worker node.
		- Log collection daemons: to grab the logs from worker and all the pods running in it.
		
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
spec:
  selector:
	matchLabels:
	  app: daemonset-nginx
  template:
	metadata:
	  labels:
		app: daemonset-nginx
	spec:
	  containers:
	  - name: nginx
		image: nginx:1.14.2
		ports:
		 - containerPort: 80
		 
Statefull Applications 
	- User session data is saved at the server side.
	- if server goes down, it is difficult to transfer the session data to other server. 
	- This type of application will not work, if we want to implement autoscaling.
	
Stateless Applications
	- user session-data is never saved at the server side.
	- using a common authentication gateway / client token method to validate the users 
	  once for multiple microservices.	
		
https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a		

Monolothic and Microservice architecture 

	Monolothic architecture
		- A monolothic application has a single code base with multiple modules in it.
		- It is a single build for entire application.
		- To make minor changes to application, we need to re-build and re-deploy the 
		  complete application.
		- scaling is very challenging.
			
	Microservice architecture 
		- A microservice application is composed of small (micro) services. 
		- Each service will have a different code base.
		- Application are divided into as small as possible sub applications called services
		  which are independent to each other which are called loosely coupled.	
		- Each service can be managed separately and it is deployable separately.
		- Services need not to share same technology stack or frameworks.		 

StatefulSet 
	- StatefulSet = Deployment + sticky identity for each and every pod replica.
	- Unlike a deployment a StatefulSet maintains a sticky identity for each of the pod.
		
Node controller 
	- Looks for node statuses and responds to API server only when a node is down.

Endpoint Controller
	- Populates the information of endpoints of all the objects.	

		
Service (svc)
	- Service is an REST api objects with set of policies for defining the 
	  access to set of pods.
	- Services are the default load balancer in k8s.
	- services are always created and works at cluster level.
	- services are the networking configurations which we do in k8s.
	- k8s prefers to use 30000 - 50000 range of ports to define services.

1. ClusterIP
	- This is the default type of service which exposes the IPs of pod to the other pods 
	  with in the same cluster.
	- ClusterIP cannot be accessed outside cluster.
	- services are the default loadbalancers of k8s.
	
	apiVersion: v1 
	kind: Service 
	metadata:
	     name: my-svc 
	spec: 
		type: ClusterIP
		selector: 
			app: my-nginx 
		ports: 
			- name: http
			  port: 30080
			  targetPort: 8080	

2. nodePort 
	- A nodeport service is the most primitive way to get the external traffic directed to our services / applications 
	  running inside a pod within the cluster.
	- By default NodePort acts as a load balancer. 
	- Automatically a ClusterIP will be created internally. 
	
		NodePort = ClusterIP + a port mapping to the all the nodes of cluster.
		
	- If we wont specify any port while creating nodeport, k8s will automatically asign a port between the range 30000 - 32767
	- By default nodeport will open the port in all the node in cluster including master node.	
	
	apiVersion: v1 
	kind: Service 
	metadata:
	     name: my-svc 
	spec: 
		type: NodePort
		selector: 
			app: my-nginx 
		ports: 
			- name: http
			  nodePort:30082	
			  port: 8080
			  targetPort: 80

		
3. Load Balancer
	- It is a type of service which is used to link external load balancer to the cluster.
	- This type of service is used by cloud providers and this service is completely depends on cloud providers. 
	- K8s now provides a better alternative for this service type which is called Ingress.


4. Headless service 
	headless service 
		  - When we neither need nor want loadbalancig and when we don't want a single IP to a service, we need to use headless service.
		  - Headless service returns all the ips of the pods it is selecting.
		  - headless service is created by specifying none for clusterIP 
		  - headless service is usually used with statefulsets.
		  
	headless with in cluster 

		apiVersion: v1 
		kind: Service 
		metadata:
			 name: my-svc 
		spec: 
			clusterIP: None
			selector: 
				app: my-nginx 
			ports: 
				- name: http
				  port: 30080
				  targetPort: 8080
				  
	headless with nodeport 			  
		nodePort = headless + port mapping 
		
		apiVersion: v1 
		kind: Service 
		metadata:
			 name: my-svc 
		spec: 
			clusterIP: None
			type: NodePort
			selector: 
				app: my-nginx 
			ports: 
				- name: http
				  nodePort:30082	
				  port: 8080
				  targetPort: 80
				  
	1. Create a headless service with statefulsets
	2. Login to any one of pod 
	3. apt install dnsutils and do nslookup <service_name>


Assignment: 
	
A. How to use custom images / connect to a registry through k8s 
	1. Login to the docker hub account 
		  docker login 
	2. Create a app to print ip 
			using flask 
	3. 	push the image to your registry 
	4 use the above custom image in k8s spec file.
	
		image: <username>/<regirty_name>:<tag>
		imagePullPolicy: IfNotPresent	
	5. Create a service of type NodePort attaching the above pods 

B. How to access application running on one pod from another pod 

C. Demo in service to service communication

D. Headless service example (Need to show the list of all pod ips) 


namespaces 
	- k8s namespaces is a way of applying abstraction / isolation to support multiple 
	  virtual clusters of k8s objects with in the same physical cluster.
	- Each and every object in k8s must be in a namespac.
	- If we wont specify namespace, objects will be created in default namespace of k8s.
    - namespaces are cluster level.
	- Namespace are only hidden from eachother but not fully isolated because one 
	  service in a namespace can talk to another service in another namespace using 
	  fullname (service/<service_name>) followed by namespace name
	
	usage: we can apply environment based logical separation on cluster. 
		
	Type of deafault NS
	1. default
	   - This NS is used for all the objects which are not belongs to any other namespace.
	   - If we wont specify any namespace while creating an object in k8s then 
         that object will be created in deafult namespace.
			
	2. kube-system 
	   - This namespace is always used for objects created by the k8s system.
	   
	3. kube-public 
	   - The objects in this namespace are available or accessable to all.
       - All the objects in this namespace are made public.

	4. kube-node-lease 
	   - This namespace holds lease objects assosiated with each node.
	   - Node lease allows the kubelet to send heartbeats so that the control palne can 
		 detect node failure.
	
	To list namespace
		kubectl get namespaces 
	
	To list objects in a namespace 
		kubectl get pods --namepsace <NS_name> 
					(OR)
		kubectl get pods -n <NS_name> 
	
	To list obects from all namespaces
		kubectl get pods --all-namespaces
		
	To create a namespace 
		kubectl create namespace <ns_name>
		
	To create k8s object in a namespace 
		1. in the spec file 
			metadata: 
				namespace: <ns_name>
				
		2. Using the apply command 
			kubectl apply -n <ns_name> -f <spec>.yml
		
		Note: what if we use both inside specfile and also in apply command 
				- apply command check and compares the namespace and wont allow to create the obejct if the namespace is different.

Assignment: try exec to a pod 				
	
	- pod to pod communication is open if the 2 pods are in the same namespace.	
	- If the pods are in different namespace by default they can't communicate we need a service object for this.
	


How a microservice will communicate with other microservice
What is service discovery in k8s 
	
Service discovery 
	There are 2 ways of dicovering a service in k8s 
	
		1. Services 
			 we can use the full name of service to discovery a microservice (pod).
			 	service/<service_name>
				
		2. DNS 
			- DNS server is added to the cluster in order to watch the k8s service request.
			- API server will create DNS records for each new service.
			- Record A type is used in k8s service discovery and this DNS is created on service
			  and pod objects.
			
			syntax of k8s DNS 
				<object_name>.<namepsace_name>.<object_type>.cluster.local
				
				ex: np-ip-app.default.svc.cluster.local
		
		3. ENV variables 
			- which ever the pod that runs on a node, k8s adds environment variables for 
			  each of them to identify the service running in it.
			  
		https://dev.to/narasimha1997/communication-between-microservices-in-a-kubernetes-cluster-1n41	



pod phases / status / states / life cycle 
	1. pending 
		- This is the status of pod when pod will be waiting for k8s cluster to accept it.
		- pod will be downloading the image from registry.
		- pod wiil be in pending till the scheduler assigns a node to the pod.
	
	2. Running 
		- The pod has been assigned a node and all the containers inside the pod is running.
		- Atleast one container is in running state and others in starting or restarting state then pod will show 
		  running state.

	3. Failed 
		- All the container in the pod should not be running and any one container being terminated in failure.
		
	4. Succeeded 
		- ALl the containers in pod have been terminated successfully/gracefully.
		
	5. Unknown 
		- For some reason the state of the pod could not be obtaied by API server.
		- The status may occur when k8s cannot communicate with the kubelet or the worker node.
	
	
terminating 
	- when pod is being deleted.
	
container status 
	Running 
		- Means container is running the process inside without any error 
	Terminated
		- Process inside the container has completed the execution or may be failed due to some error.
	waiting 
		- If a container is not running or neither in terminated state.
	
Common errors
	ImagePullBackOff 
		- Docker image registry is not accessible.
		- Image name / tag version specified is incorrent.
	CrashLoopBackOff
        - We get this error when probe check has failed.
		- Docker image may be faulty.
	RunContainerError 
		- Configmap / secrets are missing. 
		- Volumes are not available 


k8s volumes 
	persistent volume (pv)
		- It is a storage space which can be claimend to any pod in the cluster.
		- These are cluster level object and not bound to namespace.
		
		we can control the access to volume in 3 ways:
			- ReadOnlyMany(ROX) allows being mounted by multiple nodes in read-only mode.
			- ReadWriteOnce(RWO) allows being mounted by a single node in read-write mode.
			- ReadWriteMany(RWX) allows multiple nodes to be mounted in read-write mode.
		
		Note: If we need write access to volume from multiple pods scheduled in mulitple nodes then use ReadWrtieMany
		
	apiVersion: v1
	kind: PersistentVolume
	metadata:	
		name: my-pv
		labels: 
			volume: test
	spec: 
		storageClassName: local 
		accessModes: 
			- ReadWriteOnce
		capacity: 
			storage: 2Gi
		hostPath: 
		    path: "/home/ubuntu/my-pv-volume" 
			   
	Persistent volume claim (pvc)
		- This is the object used to claim / mount the required amount of storage from persistent volume to any 
		  pod in the cluster.
		- After we create the PersistentVolumeClaim, the Kubernetes control plane looks for a PersistentVolume that 
		  satisfies the claim's requirements. 
		- If the control plane finds a suitable PersistentVolume with the same StorageClass, it binds the claim to the volume.
	
	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:	
		name: my-pvc
	spec: 
		storageClassName: local 
		accessModes: 
			- ReadWriteOnce
		resources: 
		    requests:
                storage: 1Gi 
        	
	using this in a pod 
	
	apiVersion: v1
	kind: Pod 
	metadata: 
		name: my-pvc-pod	
	spec: 
        volumes: 
		    - name: pvc-volume
			  persistentVolumeClaim: 
			       claimName: my-pvc # This name should be same the PVC object name
		containers:
		   - name: my-nginx 
			 image: nginx:latest
			 ports: 
				- containerPort: 80
			 volumeMounts: 
			    - mountPath: "/usr/share/nginx/html"
				  name: pvc-volume # This name should be same as the above volume name 



Configmaps and Secrets 
	- Configmaps are k8s object that allow us to seperate the configuration data from 
	  the image content of the pod.
	- using this we can inject the environment variables to the pod containers.
    - By deafault data is not encrypted in configmaps so it is better to use these for 
	  non-confidential data.

	Create a configmap
		1. Create a file by name "app.properties"
			environment=test
			database_url="192.168.1.1"
			database_password="adjhfgjladhgalhg"
			
		2. Load the single config file 
			kubectl create configmap <configmap_name> --from-file configs/app.properties

		   Load the multiple config files 	
			kubectl create configmap <configmap_name> --from-file configs/
			
		   Create configmap spec file 
				apiVersion: v1
				kind: ConfigMap
				metadata:
				   name: test-configmap
				data:
				   environment: test
				   app: frontend
			
		3. To use configmaps to inject env varible in a pod
			
				apiVersion: v1
				kind: Pod
				metadata:
				  name: nginx-deployment-new
				spec:
				    containers:
					  - name: nginx
						image: nginx:1.14.2
						ports:
						- containerPort: 80
						env:
						  - name: CURRENT_ENV
						    valueFrom: 
                                configMapKeyRef: 							
									name: test-configmap
									key: environment
						  - name: DB_URL
						    valueFrom: 
                                configMapKeyRef: 							
									name: test-configmap
									key: database_url			
		
	SECRETS	
	
	- using secrets we can inject the environment variables to the pod containers in encrypted.
    - By deafault secrets data will be in base64 format and we use secrets for confidential data.
		

	Create a configmap
		1. Create values in base64 format 
			echo "<value>" | base64 
				output: <base64_value>
			echo "<value1>" | base64 
				output: <base64_value1>
		
		2. Load the single config file 
			kubectl create secret <configmap_name> --from-file configs/app.properties

		   Load the multiple config files 	
			kubectl create secret <configmap_name> --from-file configs/
			
		   Create configmap spec file 
				apiVersion: v1
				kind: Secret
				metadata:
				   name: test-secret
				data:
				   dburl: <base64_value>
				   dbpassword: <base64_value1>
			
		3. To use secrets to inject env varible in a pod
				apiVersion: v1
				kind: Pod
				metadata:
				  name: nginx-deployment-new
				spec:
				    containers:
					  - name: nginx
						image: nginx:1.14.2
						ports:
						- containerPort: 80
						env:
						  - name: DB_URL
						    valueFrom: 
                                secretKeyRef: 							
									name: test-secret
									key: dburl
						  - name: DB_PASSWORD
						    valueFrom: 
                                secretKeyRef: 							
									name: test-secret
									key: dbpassword		
		

	
Probes
- probe is a periodic call to some applciation endpoints within a container.
	- probes can track success or failure of the other applications.
	- When there is a subsequent failure occures we can defie probe to get triggered.
	- when subsequent success after a failure we can define probe to get triggered.
	- probes works at container level.
	
	Common fields in probes 
		initialDelaySeconds
			- After the container has started the number of seconds to wait before the probe os triggered.
		periodSeconds 
			- The number of seconds interval the probe should be executed. (Default 10 seconds and minimum 1 second)
		timeoutSeconds 
			- Number of seconds after which probe timeouts. (default 1)
			
		failureThreshold
			- When a probe fails this is the number of subsequent fail times the probe checks the status of application.
			- After the number of subsequent failure then probe fails.
			- Default value 3 with minimum value 1
		
		successThreshold 
			- minimum number of subsequent success for a probe. 
			- Default value is 1
			
		Endpoints 
			http probes (httpGet)
				host - hostname to conenct and probe will check the status of this hostname 
				     - Default is the IP of current pod 
					 ex: www.google.com
				path - exact path to access the application on the http server 
					 ex: /gmail
				httpHeaders
					- can send custom header messages with the request.
				port 
					- Name or number of the port to access the application 
					
			TCP probes 
				port 
					- Name or number of the port to access the application
			
			exec 
				commad 
				  - we execute a command and check its status.
		
Liveness probe 
		- The livenessprobe is used to determine if the applciation inside the container 
		  is healthy or needs to be restarted.	
		- If livenessprobe fails it will mark the container to be retarted by kubelet.
		
	1. LivenessPRobe with http 
		
		apiVersion: v1
		kind: Pod 
		metadata: 
			name: liveness-http	
		spec: 
			containers:
			   - name: liveness 
				 image: k8s.gcr.io/liveness
				 args:
					- /server
				 livenessProbe:
				    httpGet:
				  	   path: /healthz
				  	   port: 8080
				    initialDelaySeconds: 3
				    periodSeconds: 3	
		
	2. TCP 
		livenessProbe:
			tcpSocket:
			   port: 8080
			initialDelaySeconds: 3
			periodSeconds: 3
	3. exec 
		livenessProbe:
			exec:
			   command: ["",""]
			initialDelaySeconds: 3
			periodSeconds: 3
	
	4. named port 
		ports: 
			- name: liveness-port 
			  containerPort: 8080
              hostPort: 8080
		livenessProbe:
			httpGet:
			   path: /healthz
			   port: liveness-port
			initialDelaySeconds: 3
			periodSeconds: 3			
	
Readiness Probe 
	- ReadinessProbe is used to determine that a application running inside a 
	  container is in a state to accept the traffic.
    - When this probe is successful, the traffic from the loadbalancer is allowed 
	  to the application inside the conatiner.
	- When this probe is fails, the traffic from the loadbalancer is halted
	  to the application inside the conatiner.
	readinessProbe: 
		tcpSocket: 
			port: 8080
		initialDelaySeconds: 15
		periodSeconds: 10
		
Startup Probe 
	- This probe will run at the initial start of the container.
	- This probe allows us to give maximum startup time for application before 
	  running livenessProbe or readinessprobe.
	  
	startupProbe: 
	    httpGet: 
		   path: /healtz
		   port: 8080
		initialDelaySeconds: 3
	    periodSeconds: 3   


init container 
	- init containers are the containers that will run completely before starting 
		  the main app container.
	- This provides a lifecycle at the startup and we can define things for 
      initialization purpose.
    - kubernetes has stopped support of probes in init containers.
    - These are pod level objects.
	- we can use this container to have some deply on the startup of the main container.
	
	
	These are some of the scenarios where you can use this pattern
		- You can use this pattern where your application or main containers need some
		  prerequisites such as installing some software, database setup, permissions on the file
		  system before starting.
		- You can use this pattern where you want to delay the start of the main containers.
	
		apiVersion: v1
		kind: Pod
		metadata:
		  name: init-container
		  labels:
			 app: init-app
		spec:
		  containers:
			- name: nginx
			  image: nginx:1.14.2
			  ports:
				- containerPort: 80
			  volumeMounts:
				- mountPath: "/usr/share/nginx/html"
				  name: workdir
		  initContainers:
			- name: busybox
			  image: busybox
			  command: ["/bin/sh"]
			  args: ["-c","echo '<html><h1>I am init container new version</h1></html>' >> /work-dir/index.html"]
			  volumeMounts:
				- mountPath: "/work-dir"
				  name: workdir
		  dnsPolicy: Default
		  volumes:
			- name: workdir
			  emptyDir: {}
		
		1. login to pod 
				kubectl exec -it <pod_name> -- /bin/sh 
		2. apt update && apt install -y curl 
		3. curl localhost
			
		To check the log of particular container out of multiple in a pod 
			kubectl logs <pod_name> -c <container_name>
			
sidecar container 
	- These are the containers that will run along with the main app container.
	- we have a app conaitner which is working fine but we want to extend the 
	  functionality without changing the existing code in main container for this 
      purpose we can use sidecar container.
    - we use this container to feed the log data to monitoring tools.	
	
	These are some of the scenarios where you can use this pattern
		- Whenever you want to extend the functionality of the existing single container pod without
		  touching the existing one.
		- Whenever you want to enhance the functionality of the existing single container pod
		  without touching the existing one.
		- You can use this pattern to synchronize the main container code with the git server pull.
		- You can use this pattern for sending log events to the external server.
		- You can use this pattern for network-related tasks.
		
	apiVersion: v1
	kind: Pod
	metadata:
	  name: sidecar-container
	  labels:
		 app: adaptor-app
	spec:
	  containers:
		- name: nginx
		  image: nginx:1.14.2
		  ports:
			- containerPort: 80
		  volumeMounts:
			- mountPath: "/var/log/nginx"
			  name: logs-dir
		- name: side-car
		  image: busybox
		  command: ["/bin/sh"]
		  args: ["-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 10; done"]
		  volumeMounts:
			- mountPath: "/var/log/nginx"
			  name: logs-dir
	  dnsPolicy: Default
	  volumes:
		- name: logs-dir
		  emptyDir: {}
			  
	
Adaptor container 
	- In this patter we use a sidecar container to feed the log data to a monitoring tool.
	
	https://www.magalix.com/blog/kubernetes-patterns-the-ambassador-pattern


				
Role-Based access control 
	- accounts 
	- Roles 
	- Binding of roles 
	
	Accounts 
		1. USER ACCOUNT 
			it is used for human users to control the access to k8s.
		
		2. SERVICE ACCOUNT 
			- It is used by the applications which need the access to the cluster.
			- Any application running inside or outside the cluster need a service account.
			
			- We use a bearer token to authenticate the Service account. Beares token 
			  is created and attached to SA using secrets.
		
		To create a service account 
			kubectl create sa <account_name>
			
			Create a token for sa 
				apiVersion: v1
				kind: Secret
				metadata:
				  name: test-sa-token
				  annotations:
					kubernetes.io/service-account.name: <account_name>
				type: kubernetes.io/service-account-token
		
		use this account in an application 
			
			apiVersion: v1
			kind: Pod
			metadata:
			  name: nginx-deployment-new
			spec:
				serviceAccountName: test
			    containers:
				  - name: nginx
					image: nginx:1.14.2
					ports:
					- containerPort: 80
			
Roles 
	- For a account we can define set of rules to control the access to k8s resource.
	- Roles are always userdifined which need to be attached to a account.
	- Roles works for only namepsace. Roles are always defined for a namepsace.

	common fields in roles 
		apiGroups: List of apis to control the access
		Subject: User account, serviceaccount or Groups.
		Resources: K8S objects on which we want to define this roles
				   ex: pods, deployments etc...
		Verbs: The operations/actions that can be performed.
				ex: ["get","list","create","delete","update","watch","patch","proxy"]
			
	Create a Role 
		apiVersion: rbac.authorization.k8s.io/v1
		kind: Role
		metadata:
		  namepsace: default	
		  name: test-role
		rules:
			- apiGroups: [""]
			  resources: ["pods"]
			  verbs: ["get", "list"]	

ClusterRole 
	- this is cluster wide role 
	
	Create a ClusterRole 
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRole
		metadata:
		  name: test-cluster-role
		rules:
			- apiGroups: [""]
			  resources: ["pods"]
			  verbs: ["get", "watch", "list"]
	
Rolebinding and ClusterRoleBiniding 
	- This helps to attach a role to a subject (useraccount, serviceaccount or groups)
	- The only differenece is that we use rolebinding to attach role to account and 
	  clusterrolebinding to attach cluster role to account.
	
	- We use RoleBinding to bind a Role to a ClusterRole  
	
	RoleBinding	
		apiVersion: rbac.authorization.k8s.io/v1
		kind: RoleBinding
		metadata:
		  namespace: default
		  name: read-pods
		subjects:
		- kind: ServiceAccount
		  name: test
		  namespace: default
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: Role
		  name: test-role 
		  
	ClusterRoleBiniding	  
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  namespace: default
		  name: read-pods
		subjects:
		- kind: ServiceAccount
		  name: test
		  namespace: default
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: ClusterRole
		  name: test-role  
		  
	To check the access 
		kubectl auth can-i list pods --as=system:serviceaccount:<account_namesapce>:<account_name> 

USER ACCOUNT
1.	Create a useraccount 
	sudo useradd -s /bin/bash -d /home/<username>/ -m -G sudo <username>

2.	Create a private key for the above user created:
	sudo openssl genrsa -out <username>.key 2048

3.	Create a certificate signing request (CSR). CN is the username and O the group.
sudo openssl req -new -key <username>.key -out <username>.csr -subj "/CN=<username>/O=sudo"

4.	Sign the CSR with the Kubernetes
Note: We have to use the CA cert and key which are normally in /etc/kubernetes/pki/
	
sudo openssl x509 -req -in <username>.csr \
  	-CA /etc/kubernetes/pki/ca.crt \
  	-CAkey /etc/kubernetes/pki/ca.key \
  	-CAcreateserial \
  	-out <username>.crt -days 500

5.	Create a “.certs” directory where we are going to store the user public and private key.
sudo mkdir .certs && sudo mv jean.crt jean.key .certs

6.	Now we need to grant all the created files and directories to the user:
	sudo chown -R <username>: /home/<username>/
	sudo vi /etc/sudoers and add <username>   ALL=(ALL:ALL)  ALL	

7.	Change user to <username>
sudo -u <username> bash

8.	Create kubeconfig for <username>
kubectl config set-credentials <username> \
  --client-certificate=/home/<username>/.certs/<username>.crt \
  --client-key=/home/<username>/.certs/<username>.key

9.	Create context for <username> and cluster
kubectl config set-context <context-name> \
--cluster=<cluster-name> \
--user=<username>

10.	Switch the above created context 
	kubectl config use-context <context-name>	

11.	Edit the file $HOME/.kube/config  to Add certificate-authority-data: and server: keys under cluster. 
	apiVersion: v1
clusters:
- cluster:
   certificate-authority-data: {get from /etc/kubernetes/admin.conf}
   server: {get from /etc/kubernetes/admin.conf}
  name: kubernetes
. . . . 
12.	Check the setup by running the command 
 	Kubectl get nodes/pods
	Note: If “Error from server (Forbidden):” then create the required 
 roles/clusterRoles for the user <username>


How to configure kubectl from outside or from any other machine using service account 

1. Install kubectl on Linux
STEP1: curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl STEP2: chmod +x ./kubectl
STEP3: sudo mv ./kubectl /usr/local/bin/kubectl
To verify installation: kubectl version --client
2. Setting Kubernetes Cluster parameters
kubectl config set-cluster <cluster_name> --server=https://<IPorDNS>:<Port> --insecure-skip-tls- verify=<true/false>
Note: The above command will create the basic config file $HOME/.kube/config
To know the <IP>:<PORT>: kubectl config view
To check complete cluster activity: kubectl cluster-info (or) kubectl cluster-info dump
3. Adding the token of the user to the context
STEP 1: Copy the secret service account token from the cluster to connect
Kubectl describe serviceaccount <account_name>
STEP 2: kubectl config set-credentials <context-name> --token=<copied token>
4. Creating the Serviceaccount as User
kubectl config set-context <context-name> --cluster=<cluster_name> --user=<account_name>
5. Switch the Context to your newly created user-context
kubectl config use-context <context-name>
We can have multiple contexts with different cluster service accounts. To list contexts: kubectl config get-contexts
Best tools
1. To easy switch between context - kubectx
Get all contexts: kubectx
Switch context: kubectx <context-name>
2. To easy set namespaces default – kubens Get all contexts: kubens
Switch context: kubens <context-name>


How to create a pod on a particular worker pod 
	1. Node selector 
		- Node selector is a way of binding pod to a worker node or nodes based on the 
		  node labels.
		- We cannot use any logical expresions type of selection.  
	
		create a label to worker node 
			kubectl label node <node_name> <key>=<value>
			
		Use nodeSelector with the same label created to create pods in the same worker node
			apiVersion: apps/v1
			kind: Deployment
			metadata:
			  name: nginx-deployment
			spec:
			  replicas: 6
			  selector:
				matchLabels:
				  app: ipapp
			  template:
				metadata:
				  labels:
					app: ipapp
				spec:
				  nodeSelector:
					<key>: <value>
				  containers:
				  - name: nginx
					image: nginx:latest
					ports:
					- containerPort: 80
					
	2. Node afinity and anti-afinity (Inter-pod affinity)
		
		Node affinity 
			- nodeselector with logical expresions is affinity.
			- using node affinity we can spread pod across worker nodes based on 
			  CPU and RAM capacity(memory-intense mode), Availabilty zone (HA mode).
			
			- requiredDuringSchedulingIgnoredDuringExecution	
				The scheduler can't schedule the pod untill unless the rule is met.
				
			- preferredDuringSchedulingIgnoredDuringExecution
				The scheduler tries to fina a node matching the rule, If a matching node 
				is not available then scheduler still schedules the pod in normal way.
				
			- IgnoredDuringExecution
				If the node labels are changed after the scheduling of pod still the pod 
                continues to run. 	
			
			spec: 
				containers: 
						........
						
				affinity: 
					nodeAffinity:
					  requiredDuringSchedulingIgnoredDuringExecution:
						nodeSelectorTerms:
						- matchExpressions:
						  - key: kubernetes.io/os
							operator: In
							values:
							- linux
					  preferredDuringSchedulingIgnoredDuringExecution:
					  - weight: 1
						preference:
						  matchExpressions:
						  - key: label-1
							operator: In
							values:
							- key-1
					  - weight: 50
						preference:
						  matchExpressions:
						  - key: label-2
							operator: In
							values:
							- key-2
				
		node Anti-affinity (Inter-pod Affinity)
			- This is used to define whether a given pod should or should not be 
			  scheduled on a particular node based on conditional labels.	
			
			spec: 
				containers: 
						........
						
				affinity: 
					nodeAntiAffinity:
					  requiredDuringSchedulingIgnoredDuringExecution:
						IfNotPresent:
						- matchExpressions:
						  - key: <label_key>
							operator: In
							values:
							- <label_value>
	
	3. Taints and Tolerations
		- Taints are used to repel the pods from a specific node.
		- we can apply taints to worker nodes which tell scheduler to repel all the pods 
		  except those pods which contains tolerations for the taint.
		- 2 operators we can use Equal and Exists (If we use Exists, no value required)
		
		- We can check the effects also 
			1) NoSchedule - This taint means unless a pod with toleration the scheduler 
				will never schedule the pod.
			2) NoExecute - To delete all the pods except some reuired pods we can use this.

			To taint a worker node
				kubectl taint nodes <node_name> <taint_key>=<taint_value>:<taint_effect>
			
			To delete a taint from worker node
				(Put a "-" at the end of the command) 
				kubectl taint nodes <node_name> <taint_key>=<taint_value>:<taint_effect>-
		
		
			spec: 
				tolerations:
				  - key: env
					value: test
					effect: NoSchedule
					operator: Equal
				containers:
				  - name: nginx
					image: nginx:latest
					ports:
					- containerPort: 80



network policy
	- By default, In k8s any pod can communicate with each other within the cluster across 
	  the different namespaces and worker node.
	- The deafault netwrok of k8s is of open stack model this opens a huge risk for potential 
	  security issues.
    - We can use network polices to apply Deny all for the cluster and we can write polices 
	  to allow only required requests to cluster.
	- Network polices is defined for ingress and egress.
	
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
			name: default-deny-all
		spec: 
		    podSelector: {} 
			policyTypes: 
			    - Ingress
	
	
		apiVersion: networking.k8s.io/v1
		kind: NetworkPolicy
		metadata:
			name: nginx-pod 
		spec: 
		    podSelector: 
				matchLables:
				     app: nginx
			policyTypes: 
			    - Ingress
			ingress:
				- from: 
				   - podSelector: 
				       matchLabels: 
					       app: backend
		          ports: 
                    - port: 80
                      protocol: TCP  					
		
	calico 
		- Calico is created by a company called "Tigre". 
		- Tigre is support a wide range od CNI for kubernetes only.
		- Using this CNI plugin we can extend the usage of network policies and we can improve 
		  the security over network.	
		- Using calico we can define dynamic network policies susch auto calculated from many 
		  sources of data is possible.
		- multiple port mapping is supported in calico



Resource quotas and limits 
	How to limit the number of pods to a namepsace ?
	how to limit the memory to a pod ?
	
	Count quota 
		- This quota is used to limit the max number of obejcts that we can have 
		  in a namepsace.
		syntax: 
			count/<resource>.<group> for resources from non-core groups
			count/<resource> for resources from the core group
			
		Below are the list of resources we can have count quota 
			count/pods
			count/persistentvolumeclaims
			count/services
			count/secrets
			count/configmaps
			count/replicationcontrollers
			count/deployments.apps
			count/replicasets.apps
			count/statefulsets.apps
			count/jobs.batch
			count/cronjobs.batch
			
		ex: 
			apiVersion: v1
			kind: ResourceQuota
			metadata:
			   name: pod-count-quota
			spec:
			  hard:
				 scount/pods: "2"
				 
	Quota on CPU/RAM/Disk
		apiVersion: v1
		kind: ResourceQuota
		metadata:
		   name: resource-quota
		spec:
		  hard:
			 request.cpu: "0.2"
			 limits.cpu: "0.8"
			 
					0r 
					
			 request.memory: "512Mi"
			 limits.memory: "800Mi"	
		
		CPU 
		  - 1 cpu, in k8s 1 is equal to 100% to 1 cpu/core and 1 hyperthread.
		  - if not specified by deafult k8s allocates 0.5 cpu to a pod.
	
				we can have 0.1, 0.2, ..... 0.9, 1  
				
				0.1 cpu = 100m = 1 hundred milli cpu
		
		Memory 
			- In k8s resources are measured in bytes. 
			- The memory should in simple integer value (Fixed point number).
			- Representation Ki,Mi,Gi,Ti,Pi,Ei
					
			apiVersion: v1
			kind: Pod
			metadata:
			  name: nginx-deployment-new
			spec:
			    containers:
				  - name: nginx
					image: nginx:1.14.2
					ports:
					- containerPort: 80	
				resources:
					requests: 
						memory: "100Mi"
						cpu: 0.5
					limits:
						memory: "250Mi"
						cpu: 1


Deployment stratergies.	
	Rolling Update 
		- By default deployment in k8s uses rolling update stratergy which means if I want use 
		  this stratergy I don'nt need to specify any parameters in spec file. 
		- Example: By default k8s automatically decides the percentage of keeing available pods.
		  usually one out 4 it updates. 
		
		- To overrride the default behaviour 
			spec: 
				stratergy: 
					type: RollingUpdate 
					rollingUpdate: 
						maxSurge: 1
						maxUnavailable: 25%
			
	Recreate 
		The Recreate stratergy will bring all the old pods down immediately and the creates 
		new updated pods to match the replica count.
			spec: 
				stratergy: 
					type: Recreate 
					
	Blue/Green deployment
		- We keep 2 sets of similar environment in which one will be live called blue  
		  environment and the one which is not live is called as green. 
		- we update the new changes to green environment first which is not live and we 
		  swap/ redirect the traffic from blue to green environment.
		- Finally current green environment with new updates we rename it as blue and  
		  the current blue environment is renamed as green. 
	
	Canary release 
		- A canary release is a software testing technique used to reduce the risk of 
		  introducing a new software version into production by gradually rolling out 
		  the change to a small subgroup of users, before rolling it out to the entire 
		  platform/infrastructure.
		  

Multi master cluster 	
	What is the size of the k8s cluster ?
		- we are manitaining different cluster for different environment
		- we have one big cluster and environments are maintained through namepsaces
		- always the count of master nodes should a odd number 
		- we are using loadbalancer / multiple people are working they may dd the worker nodes 
		  so I never kept exact count of nodes but on average we have 20 to 25 worker nodes.
	
	Why always the number of master nodes is odd number ?
		- Based on Quoram calculation (n/2+1) we get same quoram failure rate for odd number 
		  and its next even number of nodes, so it is better to use odd number nodes instead 
		  of even number and we start with minimum 3 master nodes.
		- Always tell odd number of nodes (any odd number starting with 3, 5, 7, 9)
		- Based on the quorum value we choose only odd number of nodes starting with 3 to 
		  achive better fault tollerance cluster.


pod eviction 

	- Kubernetes evict pods if the node resources are running out such as cpu, RAM and storage.
	- Pod with failed state will be evicted first because they may not running but could still 
	  be using cluster resources and then k8s runs decision making based.
	  
	  Kubernetes looks at two different reasons to make eviction decision: 
			1. QoS (Quality of Service) class. 
				For every container in the pod:
				  - There must be a memory limit and a memory request.
				  - The memory limit must equal the memory request.
				  - There must be a CPU limit and a CPU request.
				  - The CPU limit must equal the CPU request.
			2. Priority class.
				- A pod's priority class defines the importance of the pod compared to other 
				  pods running in the cluster.
				- based on the priority low to high pods will be evicted.  


		  - Cluster autoscaler tools are mostly provided by public cloud providers.	

Ingress
	Cloud loadbalancer are costly as most of the times billing will be per requests so 
	to avoid this the kubernetes solution is Ingress, we can run an external software based 
	load balancer in our cluster.
		
	1. Ingress controller 
		- This controller is used to execute the ingress resources which contains 
		  routing rules and brings the external traffic based on routing rules to the 
		  internal service.	
	    - This controller will automatically monitors the exiting resources and also 
		  identifies new ingress resources.
		- This will be a third party controllers (tools) which we need to install it 
          for one time in our cluster as controller.
		- We are using nginx as ingress controller.

	2. Ingress resources 
		- In k8s Ingress resource is type of object which is used to define routing rules 
		  based on path of incoming traffic to internal cluster service.
		- api for ingress resource networking.k8s.io/v1 	
		- Ingress can be used for revers proxy means to expose multiple services under same IP.
		- Ingress can be used to apply ssl/tls certificates.

		apiVersion: networking.k8s.io/v1	
		kind: Ingress
		metadata: 
            name: my-ingress 
        spec: 
			rules: 
				- host: example.com
				    http: 
					   paths: 
					     - path: /
						   backend
				              serviceName: my-svc
							  servicePort: 80
						 - path: /login
						   backend
				              serviceName: my-login
							  servicePort: 8090 	  
							  
		Note: / 	  -	 means the request from "http://www.example.com"
			  /login  -	 means the request from "http://www.example.com/login"
			  
			  serviceName should be same as the name of the service metadata.


- The main function of a job is to create one or more pods and tracks the success 
	  status of pods.
	- Jobs ensure that the specified number of pods is completed successfully and 
	  when the job is completed pods go to the shutdown state and Job goes to completed state. 
    - Mainly we use jobs to run pod temporarily till the task is completed and to run 
	  tasks parallely.	
	  
	  apiVersion: batch/v1
	  kind: Job
	  metadata:
	    name: my-job
	  spec:
	    template:
	  	spec:
	  	  containers:
	  	  - name: busybox
	  		image: busybox
	  		command: ["echo", "This is first job"]
	  	  restartPolicy: Never

	restartPolicy 
		- This is applied to pod not for the Job.
		- There are values,
			1. Always 
			   - This is the default restart policy containers will always be restarted if they stop, 
			     even if they completed successfully. 
			   - This policy should be used for applications that always needs to be running.
			2. OnFailure
			   	  - will always restart containers only if the container process exits with an error code.
				  - If the container is determined to be unhealthy by a liveness probe it will be restarted.
				  - we use this policy for applications that need to run successfully & then stop.
			3. Never
				  - The pod’s containers will never be restarted, even if the container exits with error code 
				    or a liveness probe fails. 
	The different type of jobs or common parameters are,
	
	Completions
		- This is the number of times the job to run. default is 1.
		- If, completions is 5 then job will run 5 times means 5 pods.
	  apiVersion: batch/v1
	  kind: Job
	  metadata:
	    name: my-job
	  spec:
	  completions: 5
	    template:
	  	spec:
	  	  containers:
	  	  - name: busybox
	  		image: busybox
	  		command: ["echo", "This is first job"]
	  	  restartPolicy: Never	

	parallelism
		- By default jobs run serialy so to run jobs parallely we need to use the parallelism.
		- parallelism is used to set the number of job that need to run prallely.
		
	  apiVersion: batch/v1
	  kind: Job
	  metadata:
	    name: my-job
	  spec:
	  completions: 5
	  parallelism: 2
	    template:
	  	spec:
	  	  containers:
	  	  - name: busybox
	  		image: busybox
	  		command: ["echo", "This is first job"]
	  	  restartPolicy: Never

	backoffLimit
		- If the container is failing for some reason which affects the completion the job,
		  then still job creates more pods one after another until it succeeds which will 
		  simply put a load on the cluster, in this case backoffLimit is used.
		-  backoffLimit limit ensure the number pods to limit after failure.
		- backoffLimit: 2, once pods fails for 2 times it won’t create more pods.
	  
	  apiVersion: batch/v1
	  kind: Job
	  metadata:
	    name: my-job
	  spec:
	  backoffLimit: 2
	    template:
	  	spec:
	  	  containers:
	  	  - name: busybox
	  		image: busybox
	  		command: ["sleep", "60"]
	  	  restartPolicy: Never	

	activeDeadlineSecond
		- This is used to set the execution time for pod and if pod takes more than this 
		  deadline time then pods will be terminated automatically.

	  apiVersion: batch/v1
	  kind: Job
	  metadata:
	    name: my-job
	  spec:
	  activeDeadlineSecond: 20
	    template:
	  	spec:
	  	  containers:
	  	  - name: busybox
	  		image: busybox
	  		command: ["sleep", "60"]
	  	  restartPolicy: Never
		  
	Scheduled / CronJob
		apiVersion: batch/v1
		kind: CronJob
		metadata:
		  name: hello
		spec:
		  schedule: "* * * * *"
		  jobTemplate:
			spec:
			  template:
				spec:
				  containers:
				  - name: hello
					image: busybox:1.28
					imagePullPolicy: IfNotPresent
					command:
					- /bin/sh
					- -c
					- date; echo Hello from the Kubernetes cluster
				  restartPolicy: OnFailure
		
		1. SuccessfulJobHistoryLimit and FailedJobHistoryLimit
			apiVersion: batch/v1
			kind: CronJob
			metadata:
			  name: hello
			spec:
			  schedule: "* * * * *"
			  successfulJobHistoryLimit: 2
			  failedJobHistoryLimit: 1
			  jobTemplate:
				spec:
				  template:
					spec:
					  containers:
					  - name: hello
						image: busybox:1.28
						imagePullPolicy: IfNotPresent
						command:
						- /bin/sh
						- -c
						- date; echo Hello from the Kubernetes cluster
					  restartPolicy: OnFailure


k8s AutoScale			
	Horizontal autoscaler 
		Horizontal Pod Auto-Scaler (HPA)
			- HPA is used to automatically scale the number of pods based on deployments, replicasets, 
			  statefulsets or other objects, based on CPU, Memory threshold.
			- Automatic scaling of the horizontal pod does not apply to objects that cannot be scaled.
			  ex: DaemonSets.
			- We need metric server as a soruce for autoscalling.

			Metric server 
				- Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes 
				  API server through Metrics API for use by Horizontal Pod Autoscaler and Vertical Pod Autoscaler.
				- kubectl top command use Metrics API to list the resource utilization of all pods. 
				- Metrics Server is not meant for non-autoscaling purposes like we wont forward these metrics 
				  data to monitoring tools. 
				  
			apiVersion: autoscaling/v2beta2
			kind: HorizontalPodAutoscaler
			metadata:
			  name: php-apache-hps
			spec:
			  scaleTargetRef:
				apiVersion: apps/v1
				kind: Deployment
				name: php-apache
			  minReplicas: 1
			  maxReplicas: 10
			  metrics:
			  - type: Resource
				resource:
				  name: cpu
				  target:
					type: Utilization
					averageUtilization: 50 
					
				---------------- or ----------------
				
			kubectl autoscale deployment php-apache — cpu-percent=50 — min=1 — max=10		
			
			To list HPA 
				kubectl get hpa
			
			
		Vertical Pod Auto-Scaler (VPA)	   
		    - vpa automatically adjusts the CPU and Memory attributes for your Pods.
			- basically vpa will recreate your pod with the suitable CPU and Memory attributes.
			- when we describe vpa, it will show recommendations for the Memory/CPU requests, Limits and it can also automatically 
			  update the limits.
		  
		apiVersion: autoscaling.k8s.io/v1
		kind: VerticalPodAutoscaler
		metadata:
		  name: my-app-vpa
		spec:
		  targetRef:
			apiVersion: "apps/v1"
			kind:       Deployment
			name:       my-app
		  updatePolicy:
			updateMode: "Auto"  
			
			
		Horizontal / Vertical Cluster Auto-Scaler		
		  - Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes 
			cluster when one of the following conditions is true:
				1. some pods failed to run in the cluster due to insufficient resources,
				2. some nodes in the cluster that have been overloaded for an 
				   extended period and their pods can be placed on other existing nodes.
				   
		  - Cluster autoscaler tools are mostly provided by public cloud providers.	
